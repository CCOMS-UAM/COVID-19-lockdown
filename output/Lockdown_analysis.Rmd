---
title: 'Impact of the COVID-19 pandemic in mental health in Spain (Intro)'
subtitle: 'COVID-19 lockdown substudy within the "Edad con Salud" project'
author: Daniel Morillo
date: '`r format(lubridate::today(), "%d-%m-%Y")`'
output:
  officedown::rdocx_document:
    keep_md: no
csl:          ../../www/apa-old-doi-prefix.csl
bibliography: ../../www/Subestudio_confinamiento.bib
editor_options: 
  chunk_output_type: console
params:
  extra: no
---

```{r setup, include=FALSE}
library(pacman)
p_load(knitr)


# File system constants:

OUT_DIR  <- "output/COVID-19_lockdown"
SRC_DIR  <- "src"
SRC_FILE <- file.path(SRC_DIR, "Lockdown_analysis.R")


# Knitr output configuration:

opts_chunk$set(
  echo       = FALSE,
  results    = 'asis',
  warning    = FALSE,
  message    = FALSE,
  cache      = TRUE,
  fig.width  = 16.59/2.54,
  fig.height = 4.5
)

options(knitr.kable.NA = '')
options(digits = 3)
```


```{r read-chunks, cache=FALSE}
read_chunk(SRC_FILE)
```


```{r script-configuration, cache=FALSE}
```


```{r includes, cache=FALSE}
```


```{r constants}
```


```{r load-data}
```


```{r subset-cases}
```


```{r compute-interview-dates}
```


```{r preprocess-data}
```


```{r set-flextable-wd, cache=FALSE}
# This is necessary for flextable to work:
opts_knit$set(root.dir = file.path(getwd(), OUT_DIR))
```


# Method

We analyzed a representative sample of participants from
the communities of Madrid and Catalonia,
belonging to the update sample of the Edad con Salud project.
These participants responded to a first health interview between
`r date_init_pre` and `r date_end_pre` (Pre measure).
After the lockdown measures approved by the Spanish government in 2020,
these participants were reached out again between
`r date_init_post` and `r date_end_post`
to respond a telephonic interview (Post measure).
This second interview was intended to investigate the likely effects the
COVID-19 pandemic and lockdown measures may have had on the population.


## Outcome variables of interest

We are interested in finding relevant predictors of change in
the following variables:

- **Suicidal ideation**: This variable was measured as a single item,
taken from the Composite International Diagnostic Interview (CIDI)
for Depression Screening {== Añadir ref. (Kessler y Üstun, 2004) ==},
asking whether the participant had had suicidal thoughts.
For the Post measure, the item was adapted to ask for a 30-day time span
prior to the interview.

- **Depression**: This variable was computed with an algorithm accounting for
several factors and symptoms,
according to the Composite International Diagnostic Interview (CIDI)
for Depression Screening {== Añadir ref. (Kessler y Üstun, 2004) ==}.
The items were adapted in the Post measure to ask for a 30-day time span
prior to the interview.
Both the Pre and Post algorithms were adapted to use only the 
abbreviated item set that was used in the Post measure.

- **Sleeping time and quality**: Both measures were single items asking for
average sleeping time (in hours), and average sleeping quality
(in an ordinal scale).
While the Pre measure asked for the sleeping time and quality
the previous night,
the Post measure asked for an average of the last 30 days
prior to the interview.


## Data analysis

Descriptive statistics were obtained for each variable of interest.
Tests of change in the rate of incidence were computed for each criteria,
as corresponds to its measurement level.

Each criteria variable is then tested for association and change
in the Pre-Post measures,
using the appropriate statistical test, depending on the measurement level of
the variable.

Then, individual predictors are tested on a univariate basis, by means of a
(weighted) mixed effects model; the main effects of the measure (Pre - Post) and
the predictor, along with their interaction, are included as fixed effects;
a random effect for the participant is also specified.
This model is tested against a nested one without the interaction term, using
the likelihood ratio statistic.

{== TO BE COMPLETED ==}


# Results

## Dataset

All the outcome and other relevant variables in the Pre and Post survey were
collapsed into a single database.
Only direct (i.e. non-proxy) participants that agreed to the complete interview
in both the Pre and Post survey were considered.

Table \@ref(tab:missing-responses-table) shows the percentage of
missing responses for each variable.
The variables that had a rate of missing values higher than
`r max_missing_print` were discarded from the analyses.

<br>

```{r missing-responses}
```

{== Marta comentaba que había demasiados "missing" en `oslo3_sss_post`. Tras comprobarlo en el dataset, se debe a valores perdidos en las respuestas (`9` u `8`), así que este porcentaje de perdidos es correcto, y no son recuperables ==}


```{r missing-responses-table, tab.id="missing-responses-table", tab.cap=CAPTION_MISSING_RATES}
perc_missing %>%
  select(-value) %>%
  slice(-(1:4)) %>%
  flextable() %>%
  autofit()
```

<br>

```{r time-varying-predictors}
```


```{r subset-predictors}
```


The descriptive statistics of the predictors are represented in Tables
\@ref(tab:quantitative-descriptives-table) and
\@ref(tab:categorical-descriptives-table).
In order to achieve better numerical convergence in the optimization
algorithms,
all interval-level variables are standardized prior to data analysis.

{== TODO: Complete ==}

```{r quantitative-descriptives-table, tab.id="quantitative-descriptives-table", tab.cap=CAPTION_DESC_QUANTITATIVE_PREDICTORS}
dataset_outcomes %>%
  select(any_of(all_preds), starts_with(time_varying_preds)) %>%
  describe(skew = FALSE, omit = TRUE) %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  as_tibble() %>%
  select(-vars, -range, -se) %>%
  mutate(n = n %>% as.integer()) %>%
  mutate(across(where(is.double), number, 1e-3)) %>%
  flextable() %>%
  autofit()
```

<br>


```{r categorical-descriptives-table, tab.id="categorical-descriptives-table", tab.cap=CAPTION_DESC_CATEGORICAL_PREDICTORS}
dataset_outcomes %>%
  select(any_of(all_preds), starts_with(time_varying_preds)) %>%
  select(where(is.factor)) %>%
  frequencies_table() %>%
  flextable() %>%
  merge_v(1) %>%
  fix_border_issues() %>%
  autofit()
```

<br>


```{r standardize-predictors}
```


```{r subset-predictors-new}
```


## Sleeping time

```{r sleep-time-dataset}
```


### Descriptive statistics

```{r sleep-time-descriptives}
```


Table \@ref(tab:sleep-time-descriptives-table) shows
the descriptive statistics of the sleeping time measures
(Pre, Post, and their difference).
There may be outliers, that have sleeping times close to 0 or 24 hours
(in the Pre measure), which may be incorrect.
Figure \@ref(fig:sleep-time-histograms-figure) shows the compared histograms
of the two measures.

```{r sleep-time-descriptives-table, tab.id="sleep-time-descriptives-table", tab.cap=CAPTION_DESC_SLEEPING_TIME}
sleep_time_descriptives %>% flextable() %>% autofit()
```


```{r sleep-time-histograms}
```


```{r sleep-time-histograms-figure, fig.cap=CAPTION_HISTOGRAM_SLEEP_TIME}
sleep_time_histograms_output
```


### Population frequency estimates

The previous statistics and histogram describe the sample
but are not representative of the population as they do not take into account
sampling weights.
Table \@ref(tab:sleep-time-descriptives-est-table) and
Figure \@ref(fig:) show the equivalent estimates for the population.

<br>

```{r sleep-time-frequencies-population}
```


```{r sleep-time-descriptives-est-table, tab.id="sleep-time-descriptives-est-table", tab.cap=CAPTION_POP_EST_SLEEPING_TIME}
sleep_time_descriptives_est %>% flextable() %>% autofit()
```


```{r sleep-time-histogram-population}
```

<br>


```{r sleep-time-histogram-population-figure, fig.cap=CAPTION_POP_HISTOGRAM_SLEEP_TIME}
sleep_time_pop_hist_output
```


The distributions are similar, but the mean is higher in the Pre measure, with
much more participants reporting sleeping 8 hours.
The mode shifts from this value to 7 hours in the Post measure.
{== TODO: Correlation estimate and test do not take into account weights ==}

```{r filter-sleep-time-values}
```


In the Pre measure, there are a few cases with extreme values
(i.e., higher than 23 hours of sleep in the previous night reported).
These values are considered as outliers and filtered out from further analyses.


### Association between the Pre and Post measure

```{r cor-test-sleep-time}
```

Association between the sleeping time in the Pre and Post measure is tested with
the Pearson correlation.
The test results (`cor_test`) show that the between both measures,
though low (`cor_est_ci`),
is statistically significant.

```{r weighted-cor-test-sleep-time}
```

Similar to the frequency estimates, we compute the equivalent estimate of the
population frequency, taking into account the sampling weights.
The resulting correlation estimate `r cor_test_est` is
somewhat higher than the sample correlation, and also significant.


### Change in sleeping time

```{r change-t-test-sleep-time}
```


```{r weighted-change-t-test-sleep-time}
```


To test whether there is change in the sleeping time, we perform a
Student's paired-sample t-test.
The difference in mean sleeping time (`r diff_est_ci`) was found significant
(`r t_test`).
This implies there was an approximate decrease in average sleeping time of
`r diff_est_min` minutes.
In population terms (i.e. weighted), the mean difference estimate was of
`r diff_est_min_wtd`
minutes, lower than the in the sample but still significant (`r t_test_wtd`).


#### Univariate tests of the predictors

```{r tidy-sleep-time}
```


We compute the difference in sleeping time, and then explore its relationship
with the relevant predictors.
The difference in sleeping time is plotted against each of
the relevant quantitative predictors.

```{r quant-univariate-plots-sleep-time, results='hide', eval=params$extra}
quant_preds %>% intersect(sleep_time %>% colnames()) %>% walk(
  ~{
    plot <- sleep_time %>%
      ggplot(aes(!!sym(.x), sleeping_time_diff)) +
      geom_smooth() +
      geom_pointdensity() +
      scale_color_viridis_c() +
      theme_gray()
    
    plot %>% ggsave(
      # filename = glue(
      #   "output/COVID-19_lockdown/corrplots/Sleeping_time/{.x}.png"
      # ),
      filename = glue("corrplots/Sleeping_time/{.x}.png"),
      width = 9, height = 7.5, dpi = 1200, device = "png"
    )
  }
  
  ## None of these two considers the nature of repeated measures;
  ##   we change this by using a mixed effects model (next chunk)
  
  # Commented out code:

  # sleep_time %>%
  #   select(Diff, pred = !!sym(.x)) %$%
  #   cor.test(Diff, pred) %>%
  #   tidy()
  ## This version takes sample weights into account
  # sleep_time %>% lm(
  #   formula = glue("Diff ~ {.x}") %>% as.formula(),
  #   weights = weights
  # ) %>%
  #   tidy() %>%
  #   filter(term == .x) %>%
  #   add_column(alternative = "two sided")
)
```


```{r univariate-tests-sleep-time, results='hide'}
```


Then, we perform a (weighted) mixed effects linear model for each predictor
considered.
A model including the interaction of this model with the repeated measure
is compared with the corresponding additive model
(i.e., without the interaction term).
The difference in fit between both models is assessed
with a likelihood ratio test.
Table \@ref(tab:univariate-tests-sleep-time-results) shows the results of
the univariate tests for each predictor.

```{r univariate-tests-sleep-time-results, tab.id="univariate-tests-sleep-time-results", tab.cap=CAPTION_UNVIARIATE_TESTS_SLEEP}
# univariate_tests_sleeptime %>% bind_rows(res2_fit)
univariate_tests_sleeptime %>%
  mutate(p.value = p.value %>% format_pvalues()) %>%
  flextable() %>%
  autofit()
```

<br>


#### Multivariate model of the change in sleeping time

##### Predictor selection

```{r predictor-selection-sleep-time}
```


The predictors where the test yields a *p*-value lower than the significance
level specified of `r sig_level_print` are selected for subsequent analyses.
In order to minimize the risk of Type-II errors
(i.e., omitting predictors that could be relevant)
no multiple-comparison correction is applied to the significance level.


##### Nonlinearity tests of the ordinal predictors

```{r ordinal-linearity-tests-sleep-time}
```


The ordinal predictors selected in the previous step are tested
for nonlinearity with a similar procedure as the one used for 
predictor selection, but comparing the general model with high-order polynomic
terms with a nested model with the linear term only.
The results of these tests are given in Table
\@ref(tab:ordinal-linearity-tests-results-sleep-time).
The polynomial terms of the predictors where the test yields a *p*-value lower
than the specified significance level of `r sig_level_print` are included;
for the rest of the ordinal terms, only the linear term is taken into account.

```{r ordinal-linearity-tests-results-sleep-time, tab.id="ordinal-linearity-tests-results-sleep-time", tab.cap=CAPTION_ORDINAL_LINEARITY_TESTS_SLEEP}
ord_linearity_tests %>%
  mutate(p.value = p.value %>% format_pvalues()) %>%
  flextable() %>%
  autofit()
```

<br>


##### Linear mixed effects model fitting

A weighted mixed effects model is initially fitted to the data, with the
measure (Pre/Post) and its interaction with each predictor included as fixed
effects, along with a random intercept for each subject.
Then, terms are dropped sequentially, using the *p*-value of the F-statistic
with degrees of freedom corrected by the Satterthwaite's method
[@kuznetsova_lmertest_2017],
until no *p*-value above `r sig_level_print` (after Bonferroni correction)
is found among the potential predictors.

```{r stepwise-mixed-effects-sleep-time}
```

Table \@ref(tab:fit-history-sleep-time) shows the history of terms dropped
from the model and their statistics
(columns `ngroups` and `nobs` are the number of participants and
measures, respectively, **before** dropping the term).

```{r fit-history-sleep-time, tab.id="fit-history-sleep-time", tab.cap=CAPTION_SLEEP_FIT_HISTORY}
sleep_final_fit %>%
  attr("history") %>%
  select(term, `F value`:nobs) %>%
  flextable() %>%
  autofit()
```

<br>


After this iterative process, the final model is fit taking into account the
multilevel nature of the sample, by fitting a 2-level weighted mixed model,
using R package `WeMix` v. 3.1.5.

```{r mixed-effects-final-model-config-sleep-time}
```


```{r mixed-effects-final-model-sleep-time, eval=params$extra}
```


```{r mixed-effects-final-model-sleep-time-load, results='hide', eval=!params$extra}
load("sleep_fit.RData")
```


```{r mixed-effects-sleep-time-model-processing}
```


Table \@ref(tab:sleep-time-change-coefs) shows the statistics of the terms
for the fixed effects included in the final model,
fit with `r nobs` from `r ngroups` participants.
The model has been refit to separate the main effect of each predictor and its
interaction with the measure, which can be interpreted, respectively,
as its effect at baseline (Pre) and change explained.


```{r sleep-time-change-coefs, tab.id="sleep-time-change-coefs", tab.cap=CAPTION_SLEEP_FIT_TERMS}
sleep_coefficients %>%
  mutate(p.value = p.value %>% format_pvalues()) %>%
  flextable() %>%
  autofit()
```

<br>


##### Equivalent fit in Stata

The previous model is also fit in Stata 15, in order to check the convergence
of both procedures.
The coefficients of the model fit in Stata are given nominally,
to check that both procedures give the exact same result.

```{r mixed-effects-final-sleep-time-model-Stata}
```


```{r sleep-time-change-coefs-Stata, tab.id="sleep-time-change-coefs-Stata", tab.cap=CAPTION_SLEEP_FIT_TERMS_STATA}
stata_out_sleep_time %>%
  get_tables() %>%
  extract2(1) %>%
  mutate(
    `P>|z|` = `P>|z|` %>% format_pvalues(),
    z       = z       %>% number(1e-2),
    across(where(is.numeric), number, 1e-3)
  ) %>%
  flextable() %>%
  autofit()
```

<br>


#### Conclusions

```{r conclusions-pre-computations-sleep-time}
```


* The most relevant predictors of change in sleeping time are
`r sleep_change_sig_terms`.

* Sleeping time `r age_change_dir` by `r age_change_abs` per year
(`r age_pval`),
meaning that older adults had a higher `r age_change_dir_noun` in sleeping time.
This means that, for example, an `r AGE_MAX_EXAMPLE`-year-old person
is predicted to have lost as much as `r age_example` compared to a
`r AGE_MIN_EXAMPLE`-year-old one.

* Overall sleeping time (for 50-year-old participants, low physical activity,
and sample average values in the UCLA Loneliness scale and the
Overall Health status measure) is predicted to `r main_change_dir` by
`r main_change_abs` (`r main_pval`).

* The Post measure of the UCLA loneliness scale was a significant
predictor of change in sleep (`r ucla_lon_post_pval`),
with an estimated `r ucla_lon_post_change_dir` in sleeping time of
`r ucla_lon_post_coef_abs` per increase in standard deviation.
Or, equivalently, a `r ucla_lon_post_change_dir` in sleeping time of
`r ucla_lon_post_coef_abs_score` per point in the UCLA Loneliness scale
direct score.


##### Exploration of the relationship between age and change in sleeping time

```{r sleep-time-histogram-population-by-age}
```

The findings of the effect of age on sleeping time may be worth further
exploration.
To do this we compute and plot again a population histogram,
segmenting by age groups.
Figure \@ref(fig:sleep-time-histogram-population-by-age-figure) shows the
resulting histograms.
This shows a clear shift in the mode from 8 to 7 hours of sleep,
being the histogram of the Pre measure more negatively asymmetric.

For the older population (above 85 years old) hours of sleep tend to be
mainly between 8 and 9 in the Pre measure,
but cluster around 8 in the Post measure.
Some younger adults, in the range of 35-45, are equally distributed around
6 and 7 hours of sleep in the Post measure.
There are also a few individuals, younger than 55,
that report sleeping 11 or 12 hours a day in the Post measure.


```{r sleep-time-histogram-population-by-age-figure, fig.cap=CAPTION_POP_HISTOGRAM_SLEEP_TIME, fig.height=22.54/2.54}
sleep_time_pop_hist_by_age_output
```


## Suicidal ideation

```{r suicidal-dataset}
```


### Sample descriptives

The number and percentage of participants that reported having suicidal ideation
from the sample considered are given in
Table \@ref(tab:suicidal-descriptives-table).

```{r suicidal-descriptives}
```


```{r suicidal-descriptives-table, tab.id="suicidal-descriptives-table", tab.cap=CAPTION_DESC_SUICIDAL_IDEATION}
suicidal_props %>% flextable() %>% autofit()
```

<br>


```{r suicidal-prevalence-population}
```


The above percentage values are not weighted and thus do not correspond to
prevalence rates in the population.
The prevalence rate estimates are given in Table
\@ref(tab:suicidal-prevalence-population-table).
We can see here that the estimated prevalence decreases from
`r suicidal_prev_pre` in the Pre to `r suicidal_prev_post` in the Post measure.

```{r suicidal-prevalence-population-table, tab.id="suicidal-prevalence-population-table", tab.cap=CAPTION_PREVALENCE_SUICIDAL_IDEATION}
suicidal_prevalence_est %>% flextable() %>% autofit()
```

<br>


### Change of risk in suicidal ideation

```{r suicidal-association}
```


Table \@ref(tab:suicidal-association-table) shows the contingency table
of participants with/without suicidal ideation in the Pre and Post measures.
According to the McNemar's test of symmetry (`r mcnemar_suic`),
the sample rates seem to be significantly different
in the Pre and Post measure.

```{r suicidal-association-table, tab.id="suicidal-association-table", tab.cap=CAPTION_SUICIDAL_IDEATION_CONTINGENCY}
suic_contingency_complete %>%
  as_tibble() %>%
  flextable() %>%
  autofit()
```

<br>


```{r suicidal-association-population}
```


```{r suicidal-alluvial-population}
```


When computed applying sampling weights, the McNemar's test is not significant
(`r mcnemar_suic_est`).
This may be though a Type-II error, as its observed power is rather low
(1 - $\beta$ = `r power_test_suic`).
The estimated contingency table in the population,
where this test is computed from, is given in Table
\@ref(tab:suicidal-estimated-association-table) and represented in Figure
\@ref(fig:suicidal-alluvial-population-figure).


```{r suicidal-estimated-association-table, tab.id="suicidal-estimated-association-table", tab.cap=CAPTION_SUICIDAL_IDEATION_ESTIMATED_CONTINGENCY}
suicidal_contingency_est %>%
  adorn_pct_formatting() %>%
  flextable() %>%
  autofit()
```

<br>


```{r suicidal-alluvial-population-figure, fig.cap=CAPTION_ALLUVIAL_SUICIDAL}
suicidal_alluvial_plot
```

<br>


#### Univariate tests of the predictors

```{r tidy-suicidal}
```


```{r optimizer-tests-suicidal, results='asis', eval=params$extra}
# Testing of the optimizers in the nested model (without interaction)
optimizer_results_nested <- dataset_predictors %>%
  select_at(all_preds) %>%
  imap(
    ~{
      glmer(
        formula = glue("Suicidal ~ Measure + {.y} + (1|ID_CONTACTO)") %>%
          as.formula(),
        data    = suicidal_tidy,
        family  = binomial,
        weights = weights,
      ) %>%
        allFit()
    },
    .id = "predictor"
  )

# Get results
opts_ok <- optimizer_results_nested %>%
  map(summary) %>%
  map_dfr("which.OK", .id = "predictor")
opts_ll <- optimizer_results_nested %>%
  map(summary) %>%
  map_dfr("llik", .id = "predictor")

opts_ok %>% summarise_if(is.logical, mean)
opts_ok %>% count(nmkbw)
# Optimizer `nmkbw` has failed around 8.1% (3 out of 34), the rest have a
#   success rate of 100%

opts_tidy <- opts_ok %>%
  gather(optimizer, success, -predictor) %>%
  full_join(
    opts_ll %>% gather(optimizer, LL, -predictor),
    by = c("predictor", "optimizer")
  )

# NA values in LL imply the optimizer has failed (are omitted)
opt_LL_sd <- opts_tidy %>%
  group_by(predictor) %>%
  filter(success) %>%
  summarize(LL_sd = sd(LL, na.rm = TRUE))

# See only the ones with higher variability
opt_LL_highsd <- opt_LL_sd %>% filter(LL_sd > 1e-3)
opt_LL_highsd


best_optimizer <- opts_tidy %>%
  filter(optimizer != "nmkbw") %>%
  group_by(predictor) %>%
  slice(which.max(LL)) %>%
  select(predictor, optimizer)

# Best optimizers in general
best_optimizer %>% ungroup() %>% count(optimizer)

# Best optimizers in the predictors with more variability
best_opts_highsd <- opt_LL_highsd %>%
  left_join(best_optimizer, by = "predictor") %>%
  count(optimizer)
best_opts_highsd

chosen_opts <- best_opts_highsd %>% arrange(desc(n)) %>% slice(1:2)

# Compare each of the two chosen optimizers with the best one for the predictor:
comp_chosen_best <- opts_tidy %>%
  semi_join(chosen_opts, by = "optimizer") %>%
  bind_rows(
    opts_tidy %>% semi_join(best_optimizer, by = c("predictor", "optimizer"))
  ) %>%
  distinct() %>%
  arrange(desc(LL)) %>%
  group_by(predictor) %>%
  mutate(LL_diff = LL - first(LL)) %>%
  filter(LL_diff != 0L)

comp_chosen_best %>% filter(LL_diff %>% abs() > 1e-4)
best_optimizer %>% filter(predictor == "edu_level")

# Conclusion: `Nelder_Mead` and `nloptwrap.NLOPT_LN_NELDERMEAD` seem to give
#   almost the same results. The second one performs better in `edu_level`,
#   while they have negligible differences for the rest of the predictors.
#   Both of them are as good or almost as good as the best predictor.


# Testing of the optimizers in the general model (with interaction)
optimizer_results_general <- dataset_predictors %>%
  select_at(all_preds) %>%
  imap(
    ~{
      tryCatch(
        glmer(
          formula = glue("Suicidal ~ Measure * {.y} + (1|ID_CONTACTO)") %>%
            as.formula(),
          data    = suicidal_tidy,
          family  = binomial,
          weights = weights,
        ) %>%
          allFit(),
        error = function(e) return(NULL)
      )
    },
    .id = "predictor"
  )

# Has any default optimizer failed?
failed_opt <- optimizer_results_general %>%
  map_df(is.null) %>%
  gather(predictor, fail) %>%
  filter(fail)
failed_opt

# The default fails for this predictor, so we set the optimizer to "bobyqa"
optimizer_results_general$pain_physical <- glmer(
  formula = Suicidal ~ Measure * pain_physical + (1|ID_CONTACTO),
  data    = suicidal_tidy,
  family  = binomial,
  weights = weights,
  control = glmerControl(optimizer = "bobyqa")
) %>%
  allFit()

# Get results
opts_ok <- optimizer_results_general %>%
  map(summary) %>%
  map_dfr("which.OK", .id = "predictor")
opts_ll <- optimizer_results_general %>%
  map(summary) %>%
  map_dfr("llik", .id = "predictor")

opts_ok %>% summarise_if(is.logical, mean)
opts_ok %>% count(nmkbw)
# Optimizer `nmkbw` has failed around 19% (7 out of 37), the rest have a success
#   rate of 100%

opts_tidy <- opts_ok %>%
  gather(optimizer, success, -predictor) %>%
  full_join(
    opts_ll %>% gather(optimizer, LL, -predictor),
    by = c("predictor", "optimizer")
  )

# NA values in LL imply the optimizer has failed (are omitted)
opt_LL_sd <- opts_tidy %>%
  group_by(predictor) %>%
  filter(success) %>%
  summarize(LL_sd = sd(LL, na.rm = TRUE))

# See only the ones with higher variability
opt_LL_highsd <- opt_LL_sd %>% filter(LL_sd > 1e-3)
opt_LL_highsd


best_optimizer <- opts_tidy %>%
  filter(optimizer != "nmkbw") %>%
  group_by(predictor) %>%
  slice(which.max(LL)) %>%
  select(predictor, optimizer)

# Best optimizers in general
best_optimizer %>% ungroup() %>% count(optimizer)

# Best optimizers in the predictors with more variability
best_opts_highsd <- opt_LL_highsd %>%
  left_join(best_optimizer, by = "predictor") %>%
  count(optimizer)
best_opts_highsd

# `nloptwrap.NLOPT_LN_NELDERMEAD` is the clear first choice, in general, and
#   in the predictors with high variability in the LL.
chosen_opts <- best_opts_highsd %>% arrange(desc(n)) %>% slice(1)

# Compare the chosen optimizer with the best one for the predictor:
comp_chosen_best <- opts_tidy %>%
  semi_join(chosen_opts, by = "optimizer") %>%
  bind_rows(
    opts_tidy %>% semi_join(best_optimizer, by = c("predictor", "optimizer"))
  ) %>%
  distinct() %>%
  arrange(desc(LL)) %>%
  group_by(predictor) %>%
  mutate(LL_diff = LL - first(LL)) %>%
  filter(LL_diff != 0L)

comp_chosen_best %>% filter(LL_diff %>% abs() > 1e-4)

# Conclusion: `nloptwrap.NLOPT_LN_NELDERMEAD` is by far the best one.
#   With predictors `edu_level` and `pain_physical` there seems to be a
#   significant loss in the LL achieved.
#   We further explore the differences in these results.

edu_level_opts <- best_optimizer %>%
  ungroup() %>%
  filter(predictor == "edu_level") %>%
  bind_rows(chosen_opts) %>%
  select(optimizer)

optimizer_results_general$edu_level %>%
  summary() %>%
  extract(c("fixef", "theta")) %>%
  map(~extract(., rownames(.) %>% is_in(edu_level_opts %>% pull()), ))

# Some coefficients have high differences, even with opposite sign, but it is
#   unclear whether they are statistically significant. It may be worth testing
#   for nonlinearity first and then, if the predictor is found to be linear,
#   test again with the linear-coefficient-only version.

aux_dataset <- suicidal_tidy %>% mutate(linear = edu_level %>% as.integer())

edu_level_ordinal_fit <- glmer(
  formula = Suicidal ~ Measure * edu_level + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()
edu_level_linear_fit <- glmer(
  formula = Suicidal ~ Measure * linear + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()

map2(
  edu_level_linear_fit,
  edu_level_ordinal_fit,
  anova
) %>%
  extract(names(.) != "nmkbw") %>%
  map(tidy) %>%
  map(filter, term == ".y[[i]]") %>%
  bind_rows(.id = "optimizer") %>%
  select(optimizer, statistic:p.value)

# In all cases `edu_level` can be assumed to be linear. Although the
#   `Nelder-Mead` optimizer is far from convergence for the ordinal version,
#   it does not converge properly either, and in any case all the others tend
#   to give a similar (non-significant) result.

edu_level_linear_nested_fit <- glmer(
  formula = Suicidal ~ Measure + linear + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()

edu_level_linear_nested_fit %>% summary() %>% extract2("which.OK") # All correct
edu_level_linear_fit        %>% summary() %>% extract2("which.OK") # All correct

edu_level_linear_nested_fit %>% summary() %>% extract2("llik")
edu_level_linear_fit        %>% summary() %>% extract2("llik")

edu_level_opts_tidy <- edu_level_linear_nested_fit %>%
  summary() %>%
  extract2("llik") %>%
  enframe("optimizer", "nested") %>%
  full_join(
    edu_level_linear_fit %>%
      summary() %>%
      extract2("llik") %>%
      enframe("optimizer", "general"),
    by = "optimizer"
  ) %>%
  gather(model, LL, -optimizer) %>%
  group_by(model)

edu_level_opts_tidy %>% summarize(sd = sd(LL))
edu_level_opts_tidy %>% slice(which.max(LL))
edu_level_opts_tidy %>% mutate(diff_LL = LL - max(LL))

# Assuming `edu_level` as linear, all of the optimizers converge equally well.


aux_dataset <- suicidal_tidy %>% mutate(linear = pain_physical %>% as.integer())

pain_physical_ordinal_fit <- glmer(
  formula = Suicidal ~ Measure * pain_physical + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights,
  control = glmerControl(optimizer = "bobyqa")
) %>%
  allFit()
pain_physical_linear_fit <- glmer(
  formula = Suicidal ~ Measure * linear + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()

map2(
  pain_physical_linear_fit,
  pain_physical_ordinal_fit,
  anova
) %>%
  extract(
    pain_physical_ordinal_fit %>% # Names of the ones that did not fail in the
      summary() %>%               #   ordinal version
      extract2("which.OK") %>%
      extract(., .) %>%
      names()
  ) %>%
  map(tidy) %>%
  map(filter, term == ".y[[i]]") %>%
  bind_rows(.id = "optimizer") %>%
  select(optimizer, statistic:p.value)

# `pain_physical` cannot be assumed to be linear;
#   `nloptwrap.NLOPT_LN_NELDERMEAD` is among the optimizers that fail.

pain_physical_ordinal_fit %>%
  summary() %>%
  extract2("llik") %>%
  enframe("optimizer", "LL") %>%
  mutate(diff_LL = LL - max(LL))

# Among all the optimizers that converge, `bobyqa` is the best one.
```


```{r optimizer-configuration-suicidal}
```


We intend to perform a (weighted) generalized mixed effects linear model
for each predictor considered, with a *logit* link function.
However, this requires the use of `glmer` function from package
`lme4` (v. 1.1-23), which preforms an optimization procedure with several
alternative algorithms.
Before fitting the models to test each predictor, we explore the convergence
of the optimization algorithms.
As a result of this exhaustive exploration, we conclude that optimizer
`nloptwrap` using algorithm `NLOPT_LN_NELDERMEAD` outperforms all other
algorithms, except in the case of `pain_physical`;
in the latter, we perform the optimization using algorithm `bobyqa`.


```{r univariate-tests-suicidal, results='hide', eval=params$extra}
```


```{r univariate-tests-suicidal-load, results='hide', eval=!params$extra}
load("univariate_tests_suicidal.RData")
```


Once the optimization algorithm has been chosen for each predictor,
we fit a model including the interaction
of each predictor with the repeated measure,
and compare it against the corresponding additive model
(i.e., without the interaction term).
The difference in fit between both models is assessed
with a likelihood ratio test.
Table \@ref(tab:univariate-tests-suicidal-table) shows the results of
the univariate tests for each predictor.


```{r univariate-tests-suicidal-table, tab.id="univariate-tests-suicidal-table", tab.cap=CAPTION_UNVIARIATE_TESTS_SUICIDAL}
## Even though some predictors may give convergence issues we try to see
##   which of them are significant.
univariate_tests_suicidal %>%
  mutate(
    statistic = statistic %>% number(1e-2),
    p.value = p.value %>% format_pvalues()
  ) %>%
  flextable() %>%
  autofit()
```


<br>


#### Multivariate model of the change in suicidal ideation

##### Predictor selection

```{r predictor-selection-suicidal}
```


In order to avoid selecting too many predictors for the multivariate regression
model (which could lead to
[complete separation](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#penalizationhandling-complete-separation)),
Holm-Bonferroni correction is used to select the predictors.
This correction for multiple comparisons is more liberal than the Bonferroni
correction, thus selecting more predictors initially and allowing for a lower
Type-II error.


##### Non-linearity tests of the ordinal predictors

```{r ordinal-linearity-tests-suicidal, eval=params$extra}
```


```{r ordinal-linearity-tests-suicidal-load, results='hide', eval=!params$extra}
load("ord_linearity_tests_suicidal.RData")
```


```{r ordinal-linearity-tests-processing-suicidal}
```


The ordinal predictors selected in the previous step are tested
for non-linearity with a similar procedure as the one used for 
predictor selection, but comparing the general model with high-order polynomial
terms with a nested model with the linear term only.
The results of these tests are given in Table
\@ref(tab:ordinal-linearity-tests-results-suicidal).
The polynomial terms of the predictors where the test yields a *p*-value lower
than `r sig_level_print` (with Bonferroni correction) are included;
for the rest of the ordinal terms, only the linear term is taken into account.

```{r ordinal-linearity-tests-results-suicidal, tab.id="ordinal-linearity-tests-results-suicidal", tab.cap=CAPTION_ORDINAL_LINEARITY_TESTS_SUICIDAL}
ord_linearity_tests_suicidal %>%
  mutate(p.value = p.value %>% format_pvalues()) %>%
  flextable() %>%
  autofit()
```

<br>


##### Generalized linear mixed effects model fitting

```{r stepwise-mixed-effects-suicidal, results='hide', eval=params$extra}
```


```{r stepwise-mixed-effects-suicidal-load, results='hide', eval=!params$extra}
load("suicidal_final_fit.RData")
```


We intend to fit a generalized linear mixed effects logistic regression model
to the data, with the Measure (Pre/Post) and its interaction with
each relevant predictor included as fixed effects,
along with a random intercept for each participant.
Including all potential predictors and then dropping sequentially
the non-significant terms has the drawback of taking too long to
compute the models, because of the big size of the model matrix,
and being prone to convergence and numerical precision errors.
Instead, we fit initially a model with only the Measure and the random intercept
as predictors,
and then add sequentially each interaction term between the measure and the
selected predictor.
The criterion used to select the predictor to include in each step is the AIC
statistic.
Predictors are added while the likelihood ratio test between the models
with and without the new predictor is significant.
Table \@ref(tab:fit-history-suicidal-table) shows the history of terms added
to the model and their statistics
(columns `ngroups` and `nobs` are the number of participants and
measures, respectively, **before** adding the term).

```{r fit-history-suicidal-table, tab.id="fit-history-suicidal-table", tab.cap=CAPTION_SLEEP_FIT_HISTORY}
suicidal_final_fit %>%
  attr("history") %>%
  unnest(model_LR) %>%
  mutate(
    AIC     = candidates_AIC %>% map_dbl(min),
    p.value = p.value        %>% format_pvalues()
  ) %>%
  select(
    iteration, chosen_predictor, AIC, statistic:p.value, n_subjects, n_obs
  ) %>%
  flextable() %>%
  autofit()
```

<br>


The model fit after the iteration process has the drawback of not considering
the multilevel nature of the sampling weights though.
It also considers these weights as frequency weights instead of
sampling probability weights.
These issues make the model fit inappropriate for
parameter inference, although the iterative selection procedure for finding the
relevant predictors is still appropriate and useful.

```{r mixed-effects-final-suicidal-model-Stata}
```


The solution would be to implement the whole process in Stata.
However, Stata does not allow to implement a stepwise regression procedure as
the one described here with the `mixed` command
(the commands accepted by the stepwise procedure in Stata are listed in
the section
[Stepwise estimation](https://www.stata.com/manuals13/rstepwise.pdf)
of the Stata manual).
Therefore, once we have selected the predictors to include in the final model
with the iterative procedure described above, using R and package `lme4`,
we fit the final model with the selected predictors using Stata 15.

```{r mixed-effects-suicidal-model-processing}
```


Table \@ref(tab:fit-final-suicidal-table) shows the statistics of
the fixed effects included in the final model,
fit with `r nobs` from `r ngroups` participants.
The main effect of each predictor can be interpreted as its effect on the odds
at baseline (Pre), and its interaction with "Measure" as the change in 
the odds explained by the corresponding predictor.

```{r fit-final-suicidal-table, tab.id="fit-final-suicidal-table", tab.cap=CAPTION_SLEEP_FIT_TERMS}
suicidal_coefficients %>%
  mutate(
    `P>|z|` = `P>|z|` %>% format_pvalues(),
    z       = z       %>% number(1e-2)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


#### Conclusions

```{r conclusions-pre-computations-suicidal, cache=FALSE}
```

<br>


* The only relevant predictor of change in suicidal ideation is
`r suicidal_change_sig_terms`.
It's coefficient is `r oslo3_post_coef`,
which means a `r oslo3_post_change_dir` of a
`r oslo3_post_change_abs` per standard deviation in the odds-ratio of
having suicidal ideation.
This is equivalent to a `r oslo3_post_change_dir` of a
`r oslo3_post_neg_OR` per point in the original score of the Oslo 3
Social Support Scale.

```{r}
## TODO: Delete this?
# * The only relevant predictor of change in suicidal ideation is
# `depression_pre`.
# It's OR is `r depr_pre_coef`,
# which means a `r depr_pre_coef_dir` of a `r depr_pre_neg_OR` in the
# predicted odds-ratio of the participants with depression before the lockdown
# measures.
# 
# ```{r suicidal-depression-contingency}
# ```
# 
# Although this phenomenon seems strange, it is worth noting that the Pre
# measure of depression is more tightly associated with the Pre measure of
# Suicidal ideation, and less with the Post measure.
# Interestingly, a large amount of cases (`r suic_yes_to_no_depr_yes`)
# diagnosed with depression in the Pre
# measure transition from having to not having suicidal ideation,
# as can be seen in Table \@ref(tab:suicidal-depression-contingency-table).
# 
# In summary, it is not unreasonable, given the sample characteristics,
# that the Pre measure of depression predicts a large decrease in the risk of
# having suicidal ideation.
# However, we must be aware that this may be due to the strong association
# between the Pre measures of depression and suicidal ideation.
# 
# 
# ```{r suicidal-depression-contingency-table, tab.id="suicidal-depression-contingency-table", tab.cap=""}
# suicidal_depression_pre_contingency %>%
#   flextable() %>%
#   add_header_row(values = c("" %>% rep(2), "Depression (Pre)" %>% rep(4))) %>%
#   merge_h(1, part = "header") %>%
#   autofit()
# ```
```

<br>


### Risk of suicidal ideation after the lockdown

```{r suicidal-selection-no-pre}
```


```{r suicidal-no-pre-contingency}
```


We are interested in modelling the predicted risk of suicidal ideation in the
Post measure, in those participants that did not have suicidal ideation before
the lockdown measures had come into effect.
We first filter out the cases that had suicidal ideation in the Pre measure,
and explore the descriptives of the subsample.
Table \@ref(tab:suicidal-no-pre-contingency-table) shows the frequencies of
sample cases with/without suicidal ideation for each category
of the categorical predictors considered.

```{r suicidal-no-pre-contingency-table, tab.id="suicidal-no-pre-contingency-table", tab.cap=CAPTION_SUICIDAL_IDEATION_CONTINGENCY_PREDS_NEW}
suicidal_no_pre_contingency %>%
  flextable() %>%
  merge_v(j = 1) %>%
  fix_border_issues()
```

<br>


#### Univariate and non-linearity tests of the predictors

```{r univariate-tests-new-suicidal}
```


Then, univariate tests are performed on the Post measure for each relevant
predictor considered.
Given we don't have a repeated measure structure in this analysis,
it is easier to take the sampling design into account.
We use function `svyglm` from R package `survey` v. 3.1-12 to fit these models.
The univariate models are tested with the Rao-Scott Likelihood-ratio test
{== TODO: reference? ==}.
The results are given in Table \@ref(tab:univariate-tests-new-suicidal-table)

```{r univariate-tests-new-suicidal-table, tab.id="univariate-tests-new-suicidal-table", tab.cap=CAPTION_UNVIARIATE_TESTS_SUICIDAL_NEW}
univariate_tests_suicidal_new %>%
  mutate(
    p.value       = p.value       %>% format_pvalues(),
    lrt_Rao_Scott = lrt_Rao_Scott %>% format(digits = 3)
  ) %>%
  flextable() %>%
  autofit()
```

<br>


```{r predictor-selection-new-suicidal}
```


```{r ordinal-linearity-tests-new-suicidal}
```


The significant predictors (at `r sig_level_print`) are selected for the
subsequent multivariate regression model
In order to avoid possible Type-II errors,
no multiple-comparison correction is applied.
Then, the selected ordinal predictors are tested for non-linearity,
by comparing the general model with all the polynomial terms with a nested
model with the linear term only.
The models are compared using the Wald test
(the Likelihood Ratio test fails).
Table \@ref(tab:ordinal-linearity-tests-new-suicidal-table) shows these
contrasts. The only ordinal predictor selected is `physical_pre`, which
is non-significant (and thus only the linear term is included).

```{r ordinal-linearity-tests-new-suicidal-table, tab.id="ordinal-linearity-tests-new-suicidal-table", tab.cap=CAPTION_ORDINAL_LINEARITY_TESTS_SUICIDAL_NEW}
ord_linearity_tests_suicidal_new %>%
  mutate(
    p.value = p.value %>% format_pvalues(),
    F       = F       %>% number(1e-2)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


#### Generalized linear model fitting

```{r stepwise-glm-new-suicidal, cache=FALSE, results='hide'}
```


Then, a multivariate generalized logit linear model is fit with all the
predictors. After this, a backwards step procedure is run in order to 
selectively drop terms using the AIC statistic.
The process stops when the model in the next step does not have a smaller AIC.
Table \@ref(tab:sleep-fit-history-suicidal-new-table) shows the optimization
history, with the term dropped in each step and the AIC of the selected model.

```{r fit-history-suicidal-new-table, tab.id="sleep-fit-history-suicidal-new-table", tab.cap=CAPTION_SUICIDAL_HISTORY_NEW_FIT_HISTORY}
suicidal_final_fit_new$anova %>% flextable() %>% autofit()
```


Finally, as this stepwise procedure has to be run with a dataset without
missing values (in all potential predictor), we select the complete cases
from the predictors included in the final model, and fit it
to this dataset. This slightly improves the sample size from the last step
of the stepwise procedure, giving a sample size of `r nobs`.
The set of predictors in the final model, along with their coefficients
(log-ORs), are given in Table \@ref(tab:suicidal-coefficients-new-table).

```{r suicidal-coefficients-new-table, tab.id="suicidal-coefficients-new-table", tab.cap=CAPTION_SUICIDAL_FIT_NEW_TERMS}
suicidal_coefficients_new %>%
  mutate(
    p.value = p.value %>% format_pvalues(),
    statistic = statistic %>% number(1e-2),
    across(where(is.numeric), number, 1e-3)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


#### Conclusions

```{r conclusions-pre-computations-suicidal-new, cache=FALSE}
```


The following terms of the model are significant: `r suicidal_sig_terms_new`.

* The odds-ratio for `severity` is `r severity_coef`,
which means that a one-point increase in the scale of severity of
affliction due to the COVID-19 disease is associated with
`r severity_dir` of `r severity_abs` in the odds-ratio
of having suicidal ideation.

* The odds-ratio for `livesalone_preYes` is `r livesalone_pre_coef`,
which means that people who reported living alone before the lockdown have an
odds of having suicidal ideation a `r livesalone_pre_abs` `r livesalone_pre_dir`
than people who did not live alone before the lockdown.

* The odds-ratio for `livesalone_postYes` is `r livesalone_post_coef`,
which means that people who reported living alone during the lockdown have an
odds of having suicidal ideation a `r livesalone_post_abs`
`r livesalone_post_dir`
than people who did not live alone during the lockdown.

* The odds-ratio for `oslo3_sss_post` is `r oslo3_post_coef`,
which means that an increase of one standard deviation is associated with
`r oslo3_post_dir` of `r oslo3_post_abs` in the odds-ratio of
having suicidal ideation.
This is equivalent to `r oslo3_post_dir` of a
`r oslo3_post_neg_OR` per point in the original score of the Oslo 3
Social Support Scale.

The unlikely results found in the first three variables are probably due to
a phenomenon of complete separation, as the frequency of participants
having a severity of 1 or more, or living alone (before or after the lockdown)
is very low, and yields frequencies of zero for participants having suicidal
ideation, as can be seen in Table
\@ref(tab:complete-separation-suicidal-new-table).
Therefore, we conclude it's better to drop these predictors and run the
process again.
The new optimization history is shown in
Table \@ref(tab:fit-history-suicidal-new-table-bis)
(no terms were dropped from the model),
and the coefficients in the final model, fit with `r nobs` observations,
are given in Table \@ref(tab:suicidal-coefficients-new-table-bis).


```{r complete-separation-suicidal-new-table, tab.id="complete-separation-suicidal-new-table", tab.cap=CAPTION_SUICIDAL_NEW_COMPLETE_SEPARATION_PREDICTORS}
contingency_complete_separation_suicidal_new %>%
  flextable() %>%
  merge_v(j = 1) %>%
  autofit()
```

<br>


```{r drop-complete-separation-terms-suicidal-new}
```


```{r stepwise-glm-new-suicidal, cache=FALSE, results='hide'}
```


```{r conclusions-pre-computations-suicidal-new-bis}
```


```{r fit-history-suicidal-new-table-bis, tab.id="fit-history-suicidal-new-table-bis", tab.cap=CAPTION_SUICIDAL_HISTORY_NEW_FIT_HISTORY_WITHOUT_COMP_SEP}
suicidal_final_fit_new$anova %>% flextable() %>% autofit()
```

<br>


```{r suicidal-coefficients-new-table-bis, tab.id="suicidal-coefficients-new-table-bis", tab.cap=CAPTION_SUICIDAL_FIT_NEW_TERMS_WITHOUT_COMP_SEP}
suicidal_coefficients_new %>%
  mutate(
    p.value = p.value %>% format_pvalues(),
    statistic = statistic %>% number(1e-2),
    across(where(is.numeric), number, 1e-3)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


The only significant term now is `r suicidal_sig_terms_new`.
Its odds-ratio is `r oslo3_post_coef`,
which means that an increase of one standard deviation is associated with
`r oslo3_post_dir` of `r oslo3_post_abs` in the odds-ratio of
having suicidal ideation.
This is equivalent to `r oslo3_post_dir` of a
`r oslo3_post_neg_OR` per point in the original score of the Oslo 3
Social Support Scale.


## Depression

```{r depression-dataset}
```


### Sample descriptives

The number and percentage of participants that reported having depression
from the sample considered are given in
Table \@ref(tab:depression-descriptives-table).

```{r depression-descriptives}
```


```{r depression-descriptives-table, tab.id="depression-descriptives-table", tab.cap=CAPTION_DESC_DEPRESSION_IDEATION}
depression_props %>% flextable() %>% autofit()
```

<br>


```{r depression-prevalence-population}
```


The above percentage values are not weighted and thus do not correspond to
prevalence rates in the population.
The prevalence rate estimates are given in Table
\@ref(tab:depression-prevalence-population-table).
We can see here that the estimated prevalence increases from
`r depression_prev_pre` in the Pre to `r depression_prev_post` in the Post measure.

```{r depression-prevalence-population-table, tab.id="depression-prevalence-population-table", tab.cap=CAPTION_PREVALENCE_DEPRESSION_IDEATION}
depression_prevalence_est %>% flextable() %>% autofit()
```

<br>


### Change of risk in depression

```{r depression-association}
```


Table \@ref(tab:depression-association-table) shows the contingency table
of participants with/without depression in the Pre and Post measures.
According to the McNemar's test of symmetry (`r mcnemar_depr`),
the sample rates seem to be significantly different
in the Pre and Post measure.

```{r depression-association-table, tab.id="depression-association-table", tab.cap=CAPTION_DEPRESSION_IDEATION_CONTINGENCY}
depr_contingency_complete %>%
  as_tibble() %>%
  flextable() %>%
  autofit()
```

<br>


```{r depression-association-population}
```


```{r depression-alluvial-population}
```


When computed applying sampling weights, the McNemar's test is not significant
(`r mcnemar_depr_est`).
This may be though a Type-II error, as its observed power is rather low
(1 - $\beta$ = `r power_test_depr`).
The estimated contingency table in the population,
where this test is computed from, is given in Table
\@ref(tab:depression-estimated-association-table) and represented in Figure
\@ref(fig:depression-alluvial-population-figure).


```{r depression-estimated-association-table, tab.id="depression-estimated-association-table", tab.cap=CAPTION_DEPRESSION_IDEATION_ESTIMATED_CONTINGENCY}
depression_contingency_est %>%
  adorn_pct_formatting() %>%
  flextable() %>%
  autofit()
```

<br>


```{r depression-alluvial-population-figure, fig.cap=CAPTION_ALLUVIAL_DEPRESSION}
depression_alluvial_plot
```

<br>


#### Univariate tests of the predictors

```{r tidy-depression}
```


```{r optimizer-tests-depression, results='asis', eval=params$extra}
# Testing of the optimizers in the nested model (without interaction)
optimizer_results_nested <- dataset_predictors %>%
  select_at(all_preds) %>%
  imap(
    ~{
      glmer(
        formula = glue("Depression ~ Measure + {.y} + (1|ID_CONTACTO)") %>%
          as.formula(),
        data    = depression_tidy,
        family  = binomial,
        weights = weights,
      ) %>%
        allFit()
    },
    .id = "predictor"
  )

# Get results
opts_ok <- optimizer_results_nested %>%
  map(summary) %>%
  map_dfr("which.OK", .id = "predictor")
opts_ll <- optimizer_results_nested %>%
  map(summary) %>%
  map_dfr("llik", .id = "predictor")

opts_ok %>% summarise_if(is.logical, mean)
opts_ok %>% count(nmkbw)
# Optimizer `nmkbw` has failed around 8.1% (3 out of 34), the rest have a
#   success rate of 100%

opts_tidy <- opts_ok %>%
  gather(optimizer, success, -predictor) %>%
  full_join(
    opts_ll %>% gather(optimizer, LL, -predictor),
    by = c("predictor", "optimizer")
  )

# NA values in LL imply the optimizer has failed (are omitted)
opt_LL_sd <- opts_tidy %>%
  group_by(predictor) %>%
  filter(success) %>%
  summarize(LL_sd = sd(LL, na.rm = TRUE))

# See only the ones with higher variability
opt_LL_highsd <- opt_LL_sd %>% filter(LL_sd > 1e-3)
opt_LL_highsd


best_optimizer <- opts_tidy %>%
  filter(optimizer != "nmkbw") %>%
  group_by(predictor) %>%
  slice(which.max(LL)) %>%
  select(predictor, optimizer)

# Best optimizers in general
best_optimizer %>% ungroup() %>% count(optimizer)

# Best optimizers in the predictors with more variability
best_opts_highsd <- opt_LL_highsd %>%
  left_join(best_optimizer, by = "predictor") %>%
  count(optimizer)
best_opts_highsd

chosen_opts <- best_opts_highsd %>% arrange(desc(n)) %>% slice(1:2)

# Compare each of the two chosen optimizers with the best one for the predictor:
comp_chosen_best <- opts_tidy %>%
  semi_join(chosen_opts, by = "optimizer") %>%
  bind_rows(
    opts_tidy %>% semi_join(best_optimizer, by = c("predictor", "optimizer"))
  ) %>%
  distinct() %>%
  arrange(desc(LL)) %>%
  group_by(predictor) %>%
  mutate(LL_diff = LL - first(LL)) %>%
  filter(LL_diff != 0L)

comp_chosen_best %>% filter(LL_diff %>% abs() > 1e-4)
best_optimizer %>% filter(predictor == "edu_level")

# Conclusion: `Nelder_Mead` and `nloptwrap.NLOPT_LN_NELDERMEAD` seem to give
#   almost the same results. The second one performs better in `edu_level`,
#   while they have negligible differences for the rest of the predictors.
#   Both of them are as good or almost as good as the best predictor.


# Testing of the optimizers in the general model (with interaction)
optimizer_results_general <- dataset_predictors %>%
  select_at(all_preds) %>%
  imap(
    ~{
      tryCatch(
        glmer(
          formula = glue("Depression ~ Measure * {.y} + (1|ID_CONTACTO)") %>%
            as.formula(),
          data    = depression_tidy,
          family  = binomial,
          weights = weights,
        ) %>%
          allFit(),
        error = function(e) return(NULL)
      )
    },
    .id = "predictor"
  )

# Has any default optimizer failed?
failed_opt <- optimizer_results_general %>%
  map_df(is.null) %>%
  gather(predictor, fail) %>%
  filter(fail)
failed_opt

# The default fails for this predictor, so we set the optimizer to "bobyqa"
optimizer_results_general$pain_physical <- glmer(
  formula = Depression ~ Measure * pain_physical + (1|ID_CONTACTO),
  data    = depression_tidy,
  family  = binomial,
  weights = weights,
  control = glmerControl(optimizer = "bobyqa")
) %>%
  allFit()

# Get results
opts_ok <- optimizer_results_general %>%
  map(summary) %>%
  map_dfr("which.OK", .id = "predictor")
opts_ll <- optimizer_results_general %>%
  map(summary) %>%
  map_dfr("llik", .id = "predictor")

opts_ok %>% summarise_if(is.logical, mean)
opts_ok %>% count(nmkbw)
# Optimizer `nmkbw` has failed around 19% (7 out of 37), the rest have a success
#   rate of 100%

opts_tidy <- opts_ok %>%
  gather(optimizer, success, -predictor) %>%
  full_join(
    opts_ll %>% gather(optimizer, LL, -predictor),
    by = c("predictor", "optimizer")
  )

# NA values in LL imply the optimizer has failed (are omitted)
opt_LL_sd <- opts_tidy %>%
  group_by(predictor) %>%
  filter(success) %>%
  summarize(LL_sd = sd(LL, na.rm = TRUE))

# See only the ones with higher variability
opt_LL_highsd <- opt_LL_sd %>% filter(LL_sd > 1e-3)
opt_LL_highsd


best_optimizer <- opts_tidy %>%
  filter(optimizer != "nmkbw") %>%
  group_by(predictor) %>%
  slice(which.max(LL)) %>%
  select(predictor, optimizer)

# Best optimizers in general
best_optimizer %>% ungroup() %>% count(optimizer)

# Best optimizers in the predictors with more variability
best_opts_highsd <- opt_LL_highsd %>%
  left_join(best_optimizer, by = "predictor") %>%
  count(optimizer)
best_opts_highsd

# `nloptwrap.NLOPT_LN_NELDERMEAD` is the clear first choice, in general, and
#   in the predictors with high variability in the LL.
chosen_opts <- best_opts_highsd %>% arrange(desc(n)) %>% slice(1)

# Compare the chosen optimizer with the best one for the predictor:
comp_chosen_best <- opts_tidy %>%
  semi_join(chosen_opts, by = "optimizer") %>%
  bind_rows(
    opts_tidy %>% semi_join(best_optimizer, by = c("predictor", "optimizer"))
  ) %>%
  distinct() %>%
  arrange(desc(LL)) %>%
  group_by(predictor) %>%
  mutate(LL_diff = LL - first(LL)) %>%
  filter(LL_diff != 0L)

comp_chosen_best %>% filter(LL_diff %>% abs() > 1e-4)

# Conclusion: `nloptwrap.NLOPT_LN_NELDERMEAD` is by far the best one.
#   With predictors `edu_level` and `pain_physical` there seems to be a
#   significant loss in the LL achieved.
#   We further explore the differences in these results.

edu_level_opts <- best_optimizer %>%
  ungroup() %>%
  filter(predictor == "edu_level") %>%
  bind_rows(chosen_opts) %>%
  select(optimizer)

optimizer_results_general$edu_level %>%
  summary() %>%
  extract(c("fixef", "theta")) %>%
  map(~extract(., rownames(.) %>% is_in(edu_level_opts %>% pull()), ))

# Some coefficients have high differences, even with opposite sign, but it is
#   unclear whether they are statistically significant. It may be worth testing
#   for nonlinearity first and then, if the predictor is found to be linear,
#   test again with the linear-coefficient-only version.

aux_dataset <- depression_tidy %>% mutate(linear = edu_level %>% as.integer())

edu_level_ordinal_fit <- glmer(
  formula = Depression ~ Measure * edu_level + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()
edu_level_linear_fit <- glmer(
  formula = Depression ~ Measure * linear + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()

map2(
  edu_level_linear_fit,
  edu_level_ordinal_fit,
  anova
) %>%
  extract(names(.) != "nmkbw") %>%
  map(tidy) %>%
  map(filter, term == ".y[[i]]") %>%
  bind_rows(.id = "optimizer") %>%
  select(optimizer, statistic:p.value)

# In all cases `edu_level` can be assumed to be linear. Although the
#   `Nelder-Mead` optimizer is far from convergence for the ordinal version,
#   it does not converge properly either, and in any case all the others tend
#   to give a similar (non-significant) result.

edu_level_linear_nested_fit <- glmer(
  formula = Depression ~ Measure + linear + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()

edu_level_linear_nested_fit %>% summary() %>% extract2("which.OK") # All correct
edu_level_linear_fit        %>% summary() %>% extract2("which.OK") # All correct

edu_level_linear_nested_fit %>% summary() %>% extract2("llik")
edu_level_linear_fit        %>% summary() %>% extract2("llik")

edu_level_opts_tidy <- edu_level_linear_nested_fit %>%
  summary() %>%
  extract2("llik") %>%
  enframe("optimizer", "nested") %>%
  full_join(
    edu_level_linear_fit %>%
      summary() %>%
      extract2("llik") %>%
      enframe("optimizer", "general"),
    by = "optimizer"
  ) %>%
  gather(model, LL, -optimizer) %>%
  group_by(model)

edu_level_opts_tidy %>% summarize(sd = sd(LL))
edu_level_opts_tidy %>% slice(which.max(LL))
edu_level_opts_tidy %>% mutate(diff_LL = LL - max(LL))

# Assuming `edu_level` as linear, all of the optimizers converge equally well.


aux_dataset <- depression_tidy %>% mutate(linear = pain_physical %>% as.integer())

pain_physical_ordinal_fit <- glmer(
  formula = Depression ~ Measure * pain_physical + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights,
  control = glmerControl(optimizer = "bobyqa")
) %>%
  allFit()
pain_physical_linear_fit <- glmer(
  formula = Depression ~ Measure * linear + (1|ID_CONTACTO),
  data    = aux_dataset,
  family  = binomial,
  weights = weights
) %>%
  allFit()

map2(
  pain_physical_linear_fit,
  pain_physical_ordinal_fit,
  anova
) %>%
  extract(
    pain_physical_ordinal_fit %>% # Names of the ones that did not fail in the
      summary() %>%               #   ordinal version
      extract2("which.OK") %>%
      extract(., .) %>%
      names()
  ) %>%
  map(tidy) %>%
  map(filter, term == ".y[[i]]") %>%
  bind_rows(.id = "optimizer") %>%
  select(optimizer, statistic:p.value)

# `pain_physical` cannot be assumed to be linear;
#   `nloptwrap.NLOPT_LN_NELDERMEAD` is among the optimizers that fail.

pain_physical_ordinal_fit %>%
  summary() %>%
  extract2("llik") %>%
  enframe("optimizer", "LL") %>%
  mutate(diff_LL = LL - max(LL))

# Among all the optimizers that converge, `bobyqa` is the best one.
```


```{r optimizer-configuration-depression}
```


We intend to perform a (weighted) generalized mixed effects linear model
for each predictor considered, with a *logit* link function.
However, this requires the use of `glmer` function from package
`lme4` (v. 1.1-23), which preforms an optimization procedure with several
alternative algorithms.
Before fitting the models to test each predictor, we explore the convergence
of the optimization algorithms.
As a result of this exhaustive exploration, we conclude that optimizer
`nloptwrap` using algorithm `NLOPT_LN_NELDERMEAD` outperforms all other
algorithms, except in the case of `pain_physical`;
in the latter, we perform the optimization using algorithm `bobyqa`.


```{r univariate-tests-depression, results='hide', eval=params$extra}
```


```{r univariate-tests-depression-load, results='hide', eval=!params$extra}
load("univariate_tests_depression.RData")
```


Once the optimization algorithm has been chosen for each predictor,
we fit a model including the interaction
of each predictor with the repeated measure,
and compare it against the corresponding additive model
(i.e., without the interaction term).
The difference in fit between both models is assessed
with a likelihood ratio test.
Table \@ref(tab:univariate-tests-depression-table) shows the results of
the univariate tests for each predictor.


```{r univariate-tests-depression-table, tab.id="univariate-tests-depression-table", tab.cap=CAPTION_UNVIARIATE_TESTS_DEPRESSION}
## Even though some predictors may give convergence issues we try to see
##   which of them are significant.
univariate_tests_depression %>%
  mutate(
    statistic = statistic %>% number(1e-2),
    p.value = p.value %>% format_pvalues()
  ) %>%
  flextable() %>%
  autofit()
```


<br>


#### Multivariate model of the change in depression

##### Predictor selection

```{r predictor-selection-depression}
```


In order to avoid selecting too many predictors for the multivariate regression
model (which could lead to
[complete separation](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#penalizationhandling-complete-separation)),
Holm-Bonferroni correction is used to select the predictors.
This correction for multiple comparisons is more liberal than the Bonferroni
correction, thus selecting more predictors initially and allowing for a lower
Type-II error.


##### Non-linearity tests of the ordinal predictors

```{r ordinal-linearity-tests-depression, eval=params$extra}
```


```{r ordinal-linearity-tests-depression-load, results='hide', eval=!params$extra}
load("ord_linearity_tests_depression.RData")
```


```{r ordinal-linearity-tests-processing-depression}
```


The ordinal predictors selected in the previous step are tested
for non-linearity with a similar procedure as the one used for 
predictor selection, but comparing the general model with high-order polynomial
terms with a nested model with the linear term only.
The results of these tests are given in Table
\@ref(tab:ordinal-linearity-tests-results-depression).
The polynomial terms of the predictors where the test yields a *p*-value lower
than `r sig_level_print` (with Bonferroni correction) are included;
for the rest of the ordinal terms, only the linear term is taken into account.

```{r ordinal-linearity-tests-results-depression, tab.id="ordinal-linearity-tests-results-depression", tab.cap=CAPTION_ORDINAL_LINEARITY_TESTS_DEPRESSION}
ord_linearity_tests_depression %>%
  mutate(p.value = p.value %>% format_pvalues()) %>%
  flextable() %>%
  autofit()
```

<br>


##### Generalized linear mixed effects model fitting

```{r stepwise-mixed-effects-depression, results='hide', eval=params$extra}
```


```{r stepwise-mixed-effects-depression-load, results='hide', eval=!params$extra}
load("depression_final_fit.RData")
```


We intend to fit a generalized linear mixed effects logistic regression model
to the data, with the Measure (Pre/Post) and its interaction with
each relevant predictor included as fixed effects,
along with a random intercept for each participant.
Including all potential predictors and then dropping sequentially
the non-significant terms has the drawback of taking too long to
compute the models, because of the big size of the model matrix,
and being prone to convergence and numerical precision errors.
Instead, we fit initially a model with only the Measure and the random intercept
as predictors,
and then add sequentially each interaction term between the measure and the
selected predictor.
The criterion used to select the predictor to include in each step is the AIC
statistic.
Predictors are added while the likelihood ratio test between the models
with and without the new predictor is significant.
Table \@ref(tab:fit-history-depression-table) shows the history of terms added
to the model and their statistics
(columns `ngroups` and `nobs` are the number of participants and
measures, respectively, **before** adding the term).

```{r fit-history-depression-table, tab.id="fit-history-depression-table", tab.cap=CAPTION_SLEEP_FIT_HISTORY}
depression_final_fit %>%
  attr("history") %>%
  unnest(model_LR) %>%
  mutate(
    AIC     = candidates_AIC %>% map_dbl(min),
    p.value = p.value        %>% format_pvalues()
  ) %>%
  select(
    iteration, chosen_predictor, AIC, statistic:p.value, n_subjects, n_obs
  ) %>%
  flextable() %>%
  autofit()
```

<br>


The model fit after the iteration process has the drawback of not considering
the multilevel nature of the sampling weights though.
It also considers these weights as frequency weights instead of
sampling probability weights.
These issues make the model fit inappropriate for
parameter inference, although the iterative selection procedure for finding the
relevant predictors is still appropriate and useful.

```{r mixed-effects-final-depression-model-Stata}
```


The solution would be to implement the whole process in Stata.
However, Stata does not allow to implement a stepwise regression procedure as
the one described here with the `mixed` command
(the commands accepted by the stepwise procedure in Stata are listed in
the section
[Stepwise estimation](https://www.stata.com/manuals13/rstepwise.pdf)
of the Stata manual).
Therefore, once we have selected the predictors to include in the final model
with the iterative procedure described above, using R and package `lme4`,
we fit the final model with the selected predictors using Stata 15.

```{r mixed-effects-depression-model-processing}
```


Table \@ref(tab:fit-final-depression-table) shows the statistics of
the fixed effects included in the final model,
fit with `r nobs` from `r ngroups` participants.
The main effect of each predictor can be interpreted as its effect on the odds
at baseline (Pre), and its interaction with "Measure" as the change in 
the odds explained by the corresponding predictor.

```{r fit-final-depression-table, tab.id="fit-final-depression-table", tab.cap=CAPTION_SLEEP_FIT_TERMS}
depression_coefficients %>%
  mutate(
    `P>|z|` = `P>|z|` %>% format_pvalues(),
    z       = z       %>% number(1e-2)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


#### Conclusions

```{r conclusions-pre-computations-depression, cache=FALSE}
```

<br>


The relevant predictors of change in depression are
`r depression_change_sig_terms`.

* The odds-ratio of `health_abb_pre` is `r health_abb_pre_abs`,
which means `r health_abb_pre_dir` of a
`r health_abb_pre_incr` per standard deviation in the odds-ratio of
having depression.

* The odds-ratio of `ucla_lon_pre` is `r ucla_lon_pre_abs`,
which means `r ucla_lon_pre_dir` of a
`r ucla_lon_pre_decr` per standard deviation in the odds-ratio of
having depression.
This is equivalent to `r ucla_lon_pre_dir` of a
`r ucla_lon_pre_neg_OR` per point in the original score of the
UCLA Loneliness scale.


### Risk of depression after the lockdown

```{r depression-selection-no-pre}
```


```{r depression-no-pre-contingency}
```


We are interested in modelling the predicted risk of depression in the
Post measure, in those participants that did not have depression before
the lockdown measures had come into effect.
We first filter out the cases that had depression in the Pre measure,
and explore the descriptives of the subsample.
Table \@ref(tab:depression-no-pre-contingency-table) shows the frequencies of
sample cases with/without depression for each category
of the categorical predictors considered.

```{r depression-no-pre-contingency-table, tab.id="depression-no-pre-contingency-table", tab.cap=CAPTION_DEPRESSION_CONTINGENCY_PREDS_NEW}
depression_no_pre_contingency %>%
  flextable() %>%
  merge_v(j = 1) %>%
  fix_border_issues()
```

<br>


#### Univariate and non-linearity tests of the predictors

```{r univariate-tests-new-depression}
```


Then, univariate tests are performed on the Post measure for each relevant
predictor considered.
Given we don't have a repeated measure structure in this analysis,
it is easier to take the sampling design into account.
We use function `svyglm` from R package `survey` v. 3.1-12 to fit these models.
The univariate models are tested with the Rao-Scott Likelihood-ratio test
{== TODO: reference? ==}.
The results are given in Table \@ref(tab:univariate-tests-new-depression-table)

```{r univariate-tests-new-depression-table, tab.id="univariate-tests-new-depression-table", tab.cap=CAPTION_UNVIARIATE_TESTS_DEPRESSION_NEW}
univariate_tests_depression_new %>%
  mutate(
    p.value       = p.value       %>% format_pvalues(),
    lrt_Rao_Scott = lrt_Rao_Scott %>% format(digits = 3)
  ) %>%
  flextable() %>%
  autofit()
```

<br>


```{r predictor-selection-new-depression}
```


```{r ordinal-linearity-tests-new-depression}
```


The significant predictors (at `r sig_level_print`) are selected for the
subsequent multivariate regression model
In order to avoid possible Type-II errors,
no multiple-comparison correction is applied.
Then, the selected ordinal predictors are tested for non-linearity,
by comparing the general model with all the polynomial terms with a nested
model with the linear term only.
The models are compared using the Wald test
(the Likelihood Ratio test fails).
Table \@ref(tab:ordinal-linearity-tests-new-depression-table) shows these
contrasts. The only ordinal predictor selected is `physical_pre`, which
is non-significant (and thus only the linear term is included).

```{r ordinal-linearity-tests-new-depression-table, tab.id="ordinal-linearity-tests-new-depression-table", tab.cap=CAPTION_ORDINAL_LINEARITY_TESTS_DEPRESSION_NEW}
ord_linearity_tests_depression_new %>%
  mutate(
    p.value = p.value %>% format_pvalues(),
    F       = F       %>% number(1e-2)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


#### Generalized linear model fitting

```{r stepwise-glm-new-depression, cache=FALSE, results='hide'}
```


Then, a multivariate generalized logit linear model is fit with all the
predictors. After this, a backwards step procedure is run in order to 
selectively drop terms using the AIC statistic.
The process stops when the model in the next step does not have a smaller AIC.
Table \@ref(tab:sleep-fit-history-depression-new-table) shows the optimization
history, with the term dropped in each step and the AIC of the selected model.

```{r fit-history-depression-new-table, tab.id="sleep-fit-history-depression-new-table", tab.cap=CAPTION_DEPRESSION_HISTORY_NEW_FIT_HISTORY}
depression_final_fit_new$anova %>% flextable() %>% autofit()
```


Finally, as this stepwise procedure has to be run with a dataset without
missing values (in all potential predictor), we select the complete cases
from the predictors included in the final model, and fit it
to this dataset. This slightly improves the sample size from the last step
of the stepwise procedure, giving a sample size of `r nobs`.
The set of predictors in the final model, along with their coefficients
(log-ORs), are given in Table \@ref(tab:depression-coefficients-new-table).

```{r depression-coefficients-new-table, tab.id="depression-coefficients-new-table", tab.cap=CAPTION_DEPRESSION_FIT_NEW_TERMS}
depression_coefficients_new %>%
  mutate(
    p.value = p.value %>% format_pvalues(),
    statistic = statistic %>% number(1e-2),
    across(where(is.numeric), number, 1e-3)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


#### Conclusions

```{r conclusions-pre-computations-depression-new, cache=FALSE}
```


The following terms of the model are significant: `r depression_sig_terms_new`.

* The odds-ratio for `ucla_lon_post` is `r ucla_lon_post_OR`,
which means that an increase of one standard deviation is associated with
`r ucla_lon_post_dir` of `r ucla_lon_post_incr` in the odds-ratio of
having depression.
This is equivalent to `r ucla_lon_post_dir` of a
`r ucla_lon_post_coef_abs_score` per point in the original score of the
UCLA Loneliness Scale.

* The odds-ratio for `resilience_post` is `r resilience_post_OR`,
which means that an increase of one standard deviation is associated with
`r resilience_post_dir` of `r resilience_post_decr` in the odds-ratio of
having depression.
This is equivalent to `r resilience_post_dir` of a
`r resilience_post_coef_abs_score` per point in the original score of the
Brief Resilience Scale.

* The odds-ratio for the *Extreme* category of `pain_physical` is
`r extreme_pain_OR`,
which means that this category is associated with a probability of virtually
1 of suffering from depression.

The results in the *Extreme* category of the variable
*physical pain* are probably due to a phenomenon of complete separation,
as the frequency of participants
having Extre physical pain and no depression is zero,
as can be seen in Table \@ref(tab:complete-separation-depression-new-table).
Therefore, we conclude it's better to drop this predictor and run the
process again.
The new optimization history is shown in
Table \@ref(tab:fit-history-depression-new-table-bis),
and the coefficients in the final model, fit with `r nobs` observations,
are given in Table \@ref(tab:depression-coefficients-new-table-bis).


```{r complete-separation-depression-new-table, tab.id="complete-separation-depression-new-table", tab.cap=CAPTION_DEPRESSION_NEW_COMPLETE_SEPARATION_PREDICTORS}
contingency_complete_separation_depression_new %>%
  flextable() %>%
  merge_v(j = 1) %>%
  autofit()
```

<br>


```{r drop-complete-separation-terms-depression-new}
```


```{r stepwise-glm-new-depression, cache=FALSE, results='hide'}
```


```{r conclusions-pre-computations-depression-new-bis}
```


```{r fit-history-depression-new-table-bis, tab.id="fit-history-depression-new-table-bis", tab.cap=CAPTION_DEPRESSION_HISTORY_NEW_FIT_HISTORY_WITHOUT_COMP_SEP}
depression_final_fit_new$anova %>% flextable() %>% autofit()
```

<br>


```{r depression-coefficients-new-table-bis, tab.id="depression-coefficients-new-table-bis", tab.cap=CAPTION_DEPRESSION_FIT_NEW_TERMS_WITHOUT_COMP_SEP}
depression_coefficients_new %>%
  mutate(
    p.value = p.value %>% format_pvalues(),
    statistic = statistic %>% number(1e-2),
    across(where(is.numeric), number, 1e-3)
  ) %>%
  mutate(across(where(is.numeric), number, 1e-3)) %>% 
  flextable() %>%
  autofit()
```

<br>


The significant terms now are `r depression_sig_terms_new`.

* The odds-ratio for `ucla_lon_post` is `r ucla_lon_post_OR`,
which means that an increase of one standard deviation is associated with
`r ucla_lon_post_dir` of `r ucla_lon_post_incr` in the odds-ratio of
having depression.
This is equivalent to `r ucla_lon_post_dir` of a
`r ucla_lon_post_coef_abs_score` per point in the original score of the
UCLA Loneliness Scale.

* The odds-ratio for `resilience_post` is `r resilience_post_OR`,
which means that an increase of one standard deviation is associated with
`r resilience_post_dir` of `r resilience_post_decr` in the odds-ratio of
having depression.
This is equivalent to `r resilience_post_dir` of a
`r resilience_post_coef_abs_score` per point in the original score of the
Brief Resilience Scale.


# Methodological issues

```{r issues-pre-computations}
```


* Some excluded predictors may be included in the model as nested terms.
For example, item `SM24` (*`r sm24_label`*) is excluded due to a high rate of
missing data. However, this term only makes sense as a predictor when
`SM23` (*`r sm23_label`*) equals `1` (*Yes*).
Therefore, `SM24` may be included as a nested term within `SM23`.
This could (partially) solve some issues with high rates of missing data.

* Some predictors are considered as time-independent terms within each subject,
when they actually vary within subject from the Pre to the Post measure.
That is the case, for example, of the *Oslo 3* measure:
In order to consider both the possible effect of the Pre and Post measure,
they are included as predictors in the mixed model
However, it is obvious that the Oslo 3 Post measure does not precede
the sleeping time Pre measure.
Therefore, it does not make sense to interpret the main effect of the Oslo 3
Post measure as a predictor of sleeping time before the lockdown.
A solution may be to consider the term as variable accross measures,
when there is a Pre and a Post measure
(instead of using the Pre and the Post measure as predictors); however,
this does not allow to test the Pre measure as a predictor of change,
and the coefficient represents the effect of the *change* in the predictor
(Post value - Pre value), rather than the effect of the Post measure of the
predictor.

* Variables represented in certain arbitrary scales
(e.g. Oslo 3 scores in 0-100, or `pain_score` in direct scores)
have been standardized in order to obtain better convergence and
more interpretable coefficients.
They need to be scaled back to direct scores if necessary to interpret them.


\newpage

```{r session-info, results='hide'}
```


# References
